Currently the _1881Extractor does one full extraction 
in 609.630s (10.16m), which is too slow. 



Here is an idea for _1881Extractor-remodel that should reduce 
the extractiontime traded for increased storage used. (should be only a few KBs)
This is the idea: 



Part 1, request-n-dump:
    This part is meant to be an (almost) single-use component. 
    It will use a similar approach to how we use the industryExtractor(), 
    it will make all the requests for the industry-list-pages pr industry like usual.
    Except instead of initiating further parsing to profilePage & Regnskapstall.no It will simply just quickly dump all the links into a save-file/dump-file (dump_1) (or dump_1b if rengskapstall-link).
    The exeption being if it finds "listing-logo", it will then build and dump a link directly to renskapstall.no (skipping part two)
    <!-- dump_1 contains = list[ requests-for-industry-result-pages ] -->
    dump_1 contains = list[ all-profile-links-for-industry_1, all-profile-links-for-industry_2, ... ]
    dump_1b contains = list[ generated-regnskapstall-link-for-industry_1 ]

Part 2, ProfilePage-extractor
    Part two will read dump_1 and immediatly start requesting the profile-urls and parsing through the requests and look for signs of payed entry, if so, it will dump the regnskapstall-urls in another dump-file (dump_2).
    NOTE: if rengskapstall-link was find in dump_1 it will skip the checker and just dump the url in dump_2 right away.

Part 3, Regnskapstall-extractor
    Part three will again read the urls from dump_2 and start extracting org_num and company_name. 
    NOTE: since not all of the companies are going through the same pipeline (some are skipping steps), this could end up mixing the companies since the list is shifted. e.g.: 
        Extractor is expecting to recieve company_1 cause company_1 is supposed to be next in line, but since company_1 skipped a step, it recieves company_2 instead, thus comoany_1's info gets saved to company_2, which could lead to the whole dataset being wrong. 
        Thoguh it might not matter because dump_1 and dump_2 should only contain a link and a "bool" (not really), the "bool" being that all the links in the dump-files are in there because they are True, all False are excluded, 
        and since the dump-files contains only one input which contains unique info, and the extractors are completly dependent on that input for making an extraction and has no external variables interfering, it should not mix up the info. 
        Simply put; the extractors are not expecting anyone specific, it has a "whoever walks in that door" mentality. 

Part_1 is "single-use" because Dump_1 will never be replaced and the pages it contains will never be reextracted, part_1 will create once, then only read and appended.
In the second run it will do an lazy-update the list for changes, meaning; it only start from where it left of last time. e.g.: if the first run stopped at page=5 for "adokater" it will start from page 5.
Also, if the first element in page=5 does not have the same placement as last time, or one of the other elements (lets say company_3 is now placed on top of the list, the list has shifted by 2), then it means something has changed:
    a. 2 new companies has started paying for entries
    b. 2 companies has stopped paying for entries
Then the extactor has to trace backwards and untill it finds the changes. then it jumps over to the next industry. 

